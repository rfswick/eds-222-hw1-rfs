---
title: "EDS 222: Homework 1"
date: "10/29/2024"
author: "Rachel Swick"
execute: 
  warning: false
  message: false
---

## Background

*(The case study in this exercise is based on reality, but does not include actual observational data.)*

In this exercise we will look at a case study concerning air quality in South Asia. The World Health Organization estimates that air pollution kills an estimated seven million people per year, due to its effects on the cardiovascular and respiratory systems. Out of the 40 most polluted cities in the world, South Asia is home to 37, and Pakistan was ranked to contain the second most air pollution in the world in 2020 (IQAIR, 2020). In 2019, Lahore, Pakistan was the 12th most polluted city in the world, exposing a population of 11.1 million people to increased mortality and morbidity risks.

In this exercise, you are given two datasets from Lahore, Pakistan and are asked to compare the two different data collection strategies from this city. These data are:

-   Crowd-sourced data from air quality monitors located in people's homes. These data are voluntarily collected by individual households who choose to install a monitor in their home and upload their data for public access.

-   Official government data from monitors installed by government officials at selected locations across Lahore. There have been reports that government officials strategically locate monitors in locations with cleaner air in order to mitigate domestic and international pressure to clean up the air.

::: callout-note
All data for EDS 222 will be stored on the Taylor server, in the shared `/courses/eds-222/data/` directory. Please see material from EDS 214 on how to access and retrieve data from Taylor. These data are small; all compute can be handled locally. Thanks to Bren PhD student Fatiq Nadeem for assembling these data!
:::

In answering the following questions, please consider the lecture content from class on sampling strategies, as well as the material in Chapter 2 of [*Introduction to Modern Statistics*](https://openintro-ims.netlify.app/data-design). Include in your submission your version of this file "`eds-222-hw1.qmd`" and the rendered HTML output, each containing complete answers to all questions *as well as the associated code*. Questions with answers unsupported by the code will be marked incomplete. Showing your work this way will help you develop the habit of creating reproducible code.

## Assessment

### Question 1

Load the data from each source and label it as `crowdsourced` and `govt` accordingly. For example:


```{r}
crowdsourced <- readRDS(file.path("data", "airpol-PK-crowdsourced.RDS"))
govt <- readRDS(file.path("data", "airpol-PK-govt.RDS"))
```


```{r}
# Load libraries
library(tidyverse)
library(dplyr)

# Read in data
crowdsourced <- readRDS(file.path("data", "airpol-PK-crowdsourced.RDS"))
govt <- readRDS(file.path("data", "airpol-PK-govt.RDS"))
```

::: callout-warning
There's an implicit assumption about file organization in the code above. What is it? How can you make the code work?

The assumption is that the two data files are in a folder named "data". If the data files are not stored within a seperate folder in your R-project, only the file name would need to be included.
:::

1.  These dataframes have one row per pollution observation. How many pollution records are in each dataset?

```{r}
# Number of government pollution records
govt_records <- nrow(govt)
cat("There are", govt_records, "unique government pollution records.\n")
# Number of crowd-sourced pollution records
crowdsourced_records <- nrow(crowdsourced)
cat("There are", crowdsourced_records, "unique crowdsourced pollution records.")
```

2.  Each monitor is located at a unique latitude and longitude location. How many unique monitors are in each dataset?

```{r}
# Number of unique government monitoring locations
govt_sites <- n_distinct(govt$latitude, govt$longitude)
cat("There are", govt_sites, "unique government monitoring locations.\n")

# Number of unique crowd-sourced monitoring locations
crowdsourced_sites <- n_distinct(crowdsourced$longitude, crowdsourced$latitude)
cat("There are", crowdsourced_sites, "unique crowdsourced monitoring locations.")
```

::: callout-tip
`group_by(longitude,latitude)` and `cur_group_id()` in `dplyr` will help in creating a unique identifier for each (longitude, latitude) pair.
:::

### Question 2

The goal of pollution monitoring in Lahore is to measure the average pollution conditions across the city.

1.  What is the *population* in this setting? Please be precise. 

**The population is air quality throughout the city of Lahore from November 04, 2018 to November 30, 2019.

2.  What are the *samples* in this setting? Please be precise. 

**The fact that the data are collected with different methodologies make these separate samples, so we have a sample of crowd-sourced data and one for the government data. Both samples represent daily observations taken from November 04, 2018 to November 30, 2019.

3.  These samples were not randomly collected from across locations in Lahore. Given the sampling approaches described above, discuss possible biases that may enter when we use these samples to construct estimates of population parameters. 

The government data suffers from sampling bias because the government monitoring sites are not a random sample of air quality measures in the city of Lahore. This will likely result in the sample mean of PM 2.5 being lower than the true population mean. The crowd-sourced data also has the potential to suffer from sampling bias. Individuals who agree to have monitoring units near their homes are businesses may be incentivized to have a monitoring unit within their home or business if they know the air quality in their area is bad. The crowd-sourced mean is more likely to me closer to the true sample mean of the population.  

### Question 3

1.  For both the government data and the crowd-sourced data, report the sample mean, sample minimum, and sample maximum value of PM 2.5 (measured in $\mu g/m^3$).

```{r}
# Calculate sample means
govt_sample_mean <- mean(govt$PM)
cat("The government data has a sample mean of", govt_sample_mean, "\n")
crowdsourced_sample_mean <- mean(crowdsourced$PM)
cat("The crowdsourced data has a sample mean of", crowdsourced_sample_mean,"\n" )

# Calculate sample minimums
govt_sample_min <- min(govt$PM)
cat("The government data has a sample minimum of", govt_sample_min, "\n")
crowdsourced_sample_min <- min(crowdsourced$PM)
cat("The crowdsourced data has a sample minimum of", crowdsourced_sample_min,"\n" )

# Calculate sample maximums
govt_sample_max <- max(govt$PM)
cat("The government data has a sample max of", govt_sample_max, "\n")
crowdsourced_sample_max <- max(crowdsourced$PM)
cat("The crowdsourced data has a sample max of", crowdsourced_sample_max,"\n" )
```

2.  Discuss any key differences that you see between these two samples.

The crowd-sourced data has a larger sample mean then the government data. The sample minimum values are similar between the two data sets. The crowd-sourced data has a much higher sample maximum value then the government data.

3.  Are the differences in mean pollution as expected, given what we know about the sampling strategies?

**Yes. We expected the sample mean of the crowd-sourced data to be higher than the sample mean of the government data because the government is thought to be intentionally choosing monitoring locations with better air quality. In addition, bias from the crowd-sourced data may be artificially inflating the sample mean because the observations are likely coming from homes where residents are concerned about poor air quality.

### Question 4

Use the location of the air pollution stations for both of the sampling strategies to generate a map showing locations of each observation. Color the two samples with different colors to highlight how each sample obtains measurements from different parts of the city.

```{r}
ggplot() +
  geom_point(data = govt, 
             aes(x = longitude,
                 y = latitude,
                 colour = "Government")) +
  geom_point(data = crowdsourced, 
             aes(x = longitude, 
                 y = latitude, 
                 colour = "Crowd-sourced"))

```

::: callout-tip
`longitude` indicates location in the *x*-direction, while `latitude` indicates location in the *y*-direction. With `ggplot2` this should be nothing fancy. We'll do more spatial data in `R` later in the course.
:::

### Question 5

The local newspaper in Pakistan, *Dawn*, claims that the government is misreporting the air pollution levels in Lahore. Do the locations of monitors in question 4, relative to crowd-sourced monitors, suggest anything about a possible political bias?

Yes. The government monitoring sites are clumped near each other, while the crowd-sourced monitoring sites are spread across the city. 

### Question 6

Given the recent corruption in air quality reporting, the Prime Minister of Pakistan has hired an independent body of environmental data scientists to create an unbiased estimate of the mean PM 2.5 across Lahore using some combination of both government stations and crowd sourced observations.

NASA's satellite data indicates that the average PM across Lahore is 89.2 $\mu g/m^3$. Since this is the most objective estimate of population-level PM 2.5 available, your goal is to match this mean as closely as possible by creating a new ground-level monitoring sample that draws on both the government and crowd-sourced samples.

#### Question 6.1

First, generate a *random sample* of size $n=1000$ air pollution records by (i) pooling observations across the government and the crowd-sourced data; and (ii) drawing observations at random from this pooled sample.

```{r}
# Ensure reproducibility 
set.seed(4567)

# Combine the government and crowd-sourced datasets
combined_dataset <- bind_rows(govt, crowdsourced)

# Create a random sample of 1000 points from the combined dataset
randomized_dataset <- slice_sample(.data = combined_dataset, n = 1000)

dim(randomized_dataset)
```


::: callout-tip
`bind_rows()` may be helpful.
:::

Second, create a *stratified random sample*. Do so by (i) stratifying your pooled data-set into strata of 0.01 degrees of latitude, and (ii) randomly sampling 200 air pollution observations from each stratum.

```{r}
# Stratify the combined datset into strata of 0.01 degrees latitude
rounded_randomized_dataset <- randomized_dataset %>% 
  mutate(lat_rounded = round(latitude, digits = 2))  

# Create a rando sample of 200 points from each stratum
stratified_random_sample <- rounded_randomized_dataset %>% 
  group_by(lat_rounded) %>% 
  slice_sample(n = 200)
```

#### Question 6.2

Compare estimated means of PM 2.5 for each sampling strategy to the NASA estimate of 89.2 $\mu g/m^3$. Which sample seems to match the satellite data best? What would you recommend the Prime Minister do? Does your proposed sampling strategy rely more on government or on crowd-sourced data? Why might that be the case?

```{r}
# Compare sample means based on sampling strategy
randomized_sample_mean <- mean(randomized_dataset$PM)
cat("The random sample data has a sample mean of", randomized_sample_mean, "\n")
straifided_sample_mean <- mean(stratified_random_sample$PM)
cat("The stratified random sample data has a sample mean of", straifided_sample_mean, "\n")
```
The sample mean of the stratified random sample matches the satellite data best. I would recommend that the Prime Minister encourage residents of the city of Lahore to allow monitors to be placed near their homes and businesses. I would also recommend he discontinue the current government program entirely as it only serves to confuse the public. My proposed sampling strategy relies more on crowd-sourced data because it has a higher likelihood of being a true random sampling of the population of air quality then a government led effort. I would also recommend utilizing a stratified random sampling method as it better able to control for potentially biased data in this system.

```{r}
# Percentage of pooled data that comes from government monitors
govt_per <- (nrow(govt) / nrow(crowdsourced)) * 100
cat("Government data makes up approximately", govt_per, "percent of the pooled data.")
```

When stratifying the pooled dataset, all of the samples from government monitors are pooled into the same strata.

```{r}
# Proof all government data is in one strata
govt_strat <- govt %>% 
  mutate(lat_rounded = round(latitude, digits = 2))
n_distinct(govt_strat$lat_rounded) 
```

Since all the data from government monitoring sites is limited to one strata, and there is only 5 strata, observations from the government data is limited to only 20 percent of the sample mean. Observations from the government data are extremely biased, so limiting its influence results in a sample mean that better represents the population mean.






